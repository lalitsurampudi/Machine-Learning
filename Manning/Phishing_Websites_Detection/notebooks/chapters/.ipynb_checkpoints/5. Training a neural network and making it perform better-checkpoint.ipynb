{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final solution, we will be incorporating neural networks to see if we can boost the modeling performance. As the previous solution, we will first start by importing the necessary modules and will then load the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the uneccesary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Fix the random seed\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy arrays which will be our datasets from now\n",
    "X_train, y_train = np.load(\"X_train.npy\", allow_pickle=True), np.load(\"y_train.npy\", allow_pickle=True)\n",
    "X_test, y_test = np.load(\"X_test.npy\", allow_pickle=True), np.load(\"y_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import *\n",
    "from wandb.keras import WandbCallback\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix TensorFlow's random see for better reproducibility\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find it convenient to wrap my model definition inside a function and return a compiled version of the model from the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building using the Sequential API\n",
    "def get_training_model(data):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(40, activation=\"relu\",\n",
    "              kernel_initializer=\"uniform\",input_dim=data.shape[1]))\n",
    "    model.add(Dense(30, activation=\"relu\",\n",
    "              kernel_initializer=\"uniform\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the `ReLU` activation function it's good to use the `He_Uniform` scheme to initialize the weights of your neural network and that can be set using the `kernel_initializer` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 40)                1240      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 30)                1230      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 2,501\n",
      "Trainable params: 2,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model and print its summary\n",
    "model = get_training_model(X_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping is a good way to prevent overfitting and in many cases you won't need to train the model for all the epochs. If you see that you model's performance is not as expected the training will get stopped after fixed interval (which will be set by you) and the best model within those many epochs will be returned. Let's define an `EarlyStopping` callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_cb = EarlyStopping(monitor=\"loss\", \n",
    "    patience=5, # number of epochs to consider\n",
    "    restore_best_weights=True, # restore the best weights when loss stops enhancing\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training our shallow network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, name):\n",
    "    # Initialize Weights and Biases\n",
    "    wandb.init(project=\"phishing-websites-detection\", name=name)\n",
    "    \n",
    "    start = time.time()\n",
    "    history = model.fit(X_train, y_train, batch_size=64, epochs=128, verbose=1, \\\n",
    "        callbacks=[es_cb, WandbCallback()])\n",
    "    end = time.time()-start\n",
    "    prediction = model.predict_classes(X_test)\n",
    "    wandb.log({\"accuracy\":accuracy_score(y_test, prediction)*100.0,\\\n",
    "               \"precision\": precision_recall_fscore_support(y_test, prediction, average=\"macro\")[0],\n",
    "               \"recall\": precision_recall_fscore_support(y_test, prediction, average=\"macro\")[1],\n",
    "               \"training_time\":end})\n",
    "    print(\"Accuracy score of the Logistic Regression classifier with default hyperparameter values {0:.2f}%\"\\\n",
    "                  .format(accuracy_score(y_test, prediction)*100.))\n",
    "    print(\"\\n\")\n",
    "    print(\"----Classification report of the Logistic Regression classifier with default hyperparameter value----\")\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test, prediction, target_names=[\"Phishing Websites\", \"Normal Websites\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would encourage you to specify other things like the data, number of epochs, batch size as arguments to the function to have more flexibility and control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/sayakpaul/phishing-websites-detection\" target=\"_blank\">https://app.wandb.ai/sayakpaul/phishing-websites-detection</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/sayakpaul/phishing-websites-detection/runs/bm34c5wk\" target=\"_blank\">https://app.wandb.ai/sayakpaul/phishing-websites-detection/runs/bm34c5wk</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8844 samples\n",
      "Epoch 1/128\n",
      "8844/8844 [==============================] - 1s 95us/sample - loss: 0.3449 - accuracy: 0.8791\n",
      "Epoch 2/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.1953 - accuracy: 0.9238\n",
      "Epoch 3/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.1860 - accuracy: 0.9259\n",
      "Epoch 4/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.1754 - accuracy: 0.9287\n",
      "Epoch 5/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.1707 - accuracy: 0.9307\n",
      "Epoch 6/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.1619 - accuracy: 0.9342\n",
      "Epoch 7/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.1518 - accuracy: 0.9383\n",
      "Epoch 8/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.1444 - accuracy: 0.9392\n",
      "Epoch 9/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.1384 - accuracy: 0.9419\n",
      "Epoch 10/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.1320 - accuracy: 0.9450\n",
      "Epoch 11/128\n",
      "8844/8844 [==============================] - 0s 34us/sample - loss: 0.1268 - accuracy: 0.9469\n",
      "Epoch 12/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.1223 - accuracy: 0.9486\n",
      "Epoch 13/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.1178 - accuracy: 0.9491\n",
      "Epoch 14/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.1170 - accuracy: 0.9507\n",
      "Epoch 15/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.1133 - accuracy: 0.9526\n",
      "Epoch 16/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.1085 - accuracy: 0.9545\n",
      "Epoch 17/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.1092 - accuracy: 0.9521\n",
      "Epoch 18/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.1072 - accuracy: 0.9545\n",
      "Epoch 19/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.1031 - accuracy: 0.9559\n",
      "Epoch 20/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.1026 - accuracy: 0.9560\n",
      "Epoch 21/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0977 - accuracy: 0.9579\n",
      "Epoch 22/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0982 - accuracy: 0.9579\n",
      "Epoch 23/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0957 - accuracy: 0.9596\n",
      "Epoch 24/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0929 - accuracy: 0.9603\n",
      "Epoch 25/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0940 - accuracy: 0.9595\n",
      "Epoch 26/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0918 - accuracy: 0.9616\n",
      "Epoch 27/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0917 - accuracy: 0.9600\n",
      "Epoch 28/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0916 - accuracy: 0.9610\n",
      "Epoch 29/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0871 - accuracy: 0.9633\n",
      "Epoch 30/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0919 - accuracy: 0.9628\n",
      "Epoch 31/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0841 - accuracy: 0.9653\n",
      "Epoch 32/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0851 - accuracy: 0.9625\n",
      "Epoch 33/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0810 - accuracy: 0.9662\n",
      "Epoch 34/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0833 - accuracy: 0.9631\n",
      "Epoch 35/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0791 - accuracy: 0.9669\n",
      "Epoch 36/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0835 - accuracy: 0.9648\n",
      "Epoch 37/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0788 - accuracy: 0.9671\n",
      "Epoch 38/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0771 - accuracy: 0.9678\n",
      "Epoch 39/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0784 - accuracy: 0.9664\n",
      "Epoch 40/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0753 - accuracy: 0.9675\n",
      "Epoch 41/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0722 - accuracy: 0.9695\n",
      "Epoch 42/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.0711 - accuracy: 0.9715\n",
      "Epoch 43/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0759 - accuracy: 0.9686\n",
      "Epoch 44/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0692 - accuracy: 0.9703\n",
      "Epoch 45/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0696 - accuracy: 0.9723\n",
      "Epoch 46/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0684 - accuracy: 0.9700\n",
      "Epoch 47/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0660 - accuracy: 0.9725\n",
      "Epoch 48/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0679 - accuracy: 0.9698\n",
      "Epoch 49/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0678 - accuracy: 0.9713\n",
      "Epoch 50/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0642 - accuracy: 0.9722\n",
      "Epoch 51/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0642 - accuracy: 0.9725\n",
      "Epoch 52/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0631 - accuracy: 0.9735\n",
      "Epoch 53/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0665 - accuracy: 0.9716\n",
      "Epoch 54/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0623 - accuracy: 0.9730\n",
      "Epoch 55/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0606 - accuracy: 0.9746\n",
      "Epoch 56/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0662 - accuracy: 0.9718\n",
      "Epoch 57/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0644 - accuracy: 0.9722\n",
      "Epoch 58/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0605 - accuracy: 0.9737\n",
      "Epoch 59/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0581 - accuracy: 0.9767\n",
      "Epoch 60/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.0599 - accuracy: 0.9744\n",
      "Epoch 61/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0583 - accuracy: 0.9749\n",
      "Epoch 62/128\n",
      "8844/8844 [==============================] - 0s 38us/sample - loss: 0.0587 - accuracy: 0.9748\n",
      "Epoch 63/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0562 - accuracy: 0.9766\n",
      "Epoch 64/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0590 - accuracy: 0.9751\n",
      "Epoch 65/128\n",
      "8844/8844 [==============================] - 0s 37us/sample - loss: 0.0573 - accuracy: 0.9755\n",
      "Epoch 66/128\n",
      "8844/8844 [==============================] - 0s 31us/sample - loss: 0.0618 - accuracy: 0.9732\n",
      "Epoch 67/128\n",
      "8844/8844 [==============================] - 0s 36us/sample - loss: 0.0557 - accuracy: 0.9756\n",
      "Epoch 68/128\n",
      "8844/8844 [==============================] - 0s 34us/sample - loss: 0.0561 - accuracy: 0.9759\n",
      "Epoch 69/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.0556 - accuracy: 0.9763\n",
      "Epoch 70/128\n",
      "8844/8844 [==============================] - 0s 37us/sample - loss: 0.0555 - accuracy: 0.9782\n",
      "Epoch 71/128\n",
      "8844/8844 [==============================] - 0s 30us/sample - loss: 0.0598 - accuracy: 0.9734\n",
      "Epoch 72/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.0572 - accuracy: 0.9756\n",
      "Epoch 73/128\n",
      "8844/8844 [==============================] - 0s 45us/sample - loss: 0.0596 - accuracy: 0.9743\n",
      "Epoch 74/128\n",
      "8844/8844 [==============================] - 0s 40us/sample - loss: 0.0540 - accuracy: 0.9759\n",
      "Epoch 75/128\n",
      "8844/8844 [==============================] - 0s 40us/sample - loss: 0.0566 - accuracy: 0.9758\n",
      "Epoch 76/128\n",
      "8844/8844 [==============================] - 0s 35us/sample - loss: 0.0530 - accuracy: 0.9782\n",
      "Epoch 77/128\n",
      "8844/8844 [==============================] - 0s 46us/sample - loss: 0.0539 - accuracy: 0.9764\n",
      "Epoch 78/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0526 - accuracy: 0.9780s - loss: 0.0530 - accuracy: 0.97\n",
      "Epoch 79/128\n",
      "8844/8844 [==============================] - 0s 27us/sample - loss: 0.0521 - accuracy: 0.9773\n",
      "Epoch 80/128\n",
      "8844/8844 [==============================] - 0s 26us/sample - loss: 0.0505 - accuracy: 0.9783\n",
      "Epoch 81/128\n",
      "8844/8844 [==============================] - 0s 28us/sample - loss: 0.0520 - accuracy: 0.9778\n",
      "Epoch 82/128\n",
      "8844/8844 [==============================] - 0s 27us/sample - loss: 0.0517 - accuracy: 0.9772\n",
      "Epoch 83/128\n",
      "8844/8844 [==============================] - 0s 28us/sample - loss: 0.0523 - accuracy: 0.9781\n",
      "Epoch 84/128\n",
      "8844/8844 [==============================] - 0s 32us/sample - loss: 0.0495 - accuracy: 0.9782\n",
      "Epoch 85/128\n",
      "8844/8844 [==============================] - 0s 43us/sample - loss: 0.0507 - accuracy: 0.9782\n",
      "Epoch 86/128\n",
      "8844/8844 [==============================] - 0s 34us/sample - loss: 0.0495 - accuracy: 0.9799\n",
      "Epoch 87/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.0505 - accuracy: 0.9791\n",
      "Epoch 88/128\n",
      "8844/8844 [==============================] - 0s 34us/sample - loss: 0.0487 - accuracy: 0.9785\n",
      "Epoch 89/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.0517 - accuracy: 0.9761\n",
      "Epoch 90/128\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.0491 - accuracy: 0.9783\n",
      "Epoch 91/128\n",
      "8844/8844 [==============================] - 0s 34us/sample - loss: 0.0510 - accuracy: 0.9764\n",
      "Epoch 92/128\n",
      "8844/8844 [==============================] - 0s 34us/sample - loss: 0.0503 - accuracy: 0.9772\n",
      "Epoch 93/128\n",
      "8000/8844 [==========================>...] - ETA: 0s - loss: 0.0509 - accuracy: 0.9759Restoring model weights from the end of the best epoch.\n",
      "8844/8844 [==============================] - 0s 33us/sample - loss: 0.0497 - accuracy: 0.9763\n",
      "Epoch 00093: early stopping\n",
      "Accuracy score of the Logistic Regression classifier with default hyperparameter values 97.93%\n",
      "\n",
      "\n",
      "----Classification report of the Logistic Regression classifier with default hyperparameter value----\n",
      "\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Phishing Websites       0.97      0.99      0.98      3924\n",
      "  Normal Websites       0.99      0.97      0.98      4920\n",
      "\n",
      "        micro avg       0.98      0.98      0.98      8844\n",
      "        macro avg       0.98      0.98      0.98      8844\n",
      "     weighted avg       0.98      0.98      0.98      8844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_network(model, \"neural-network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have a training accuracy of 97%. Can we push this even further? \n",
    "\n",
    "**TDLHBA** is technique introduced [in this paper](https://dl.acm.org/citation.cfm?id=3227655). We will use hyperparameter values as presented in the paper to see the performance enhancement of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model with the same topology as specified in the above-mentioned paper\n",
    "\n",
    "model_TDLHBA = Sequential()\n",
    "\n",
    "model_TDLHBA.add(Dense(40, activation=\"relu\",\n",
    "          kernel_initializer=\"uniform\", input_dim=X_train.shape[1]))\n",
    "model_TDLHBA.add(Dense(30, activation=\"relu\",\n",
    "          kernel_initializer=\"uniform\"))\n",
    "model_TDLHBA.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "adam = Adam(lr=0.0017470)\n",
    "model_TDLHBA.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/sayakpaul/phishing-websites-detection\" target=\"_blank\">https://app.wandb.ai/sayakpaul/phishing-websites-detection</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/sayakpaul/phishing-websites-detection/runs/1hz6kgbw\" target=\"_blank\">https://app.wandb.ai/sayakpaul/phishing-websites-detection/runs/1hz6kgbw</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/sayakpaul/phishing-websites-detection/runs/1hz6kgbw"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"phishing-websites-detection\", name=\"neural-network-tdlhba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8844 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1220 22:34:05.154269 4382574016 callbacks.py:244] Method (on_train_batch_end) is slow compared to the batch update (2.423479). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  10/8844 [..............................] - ETA: 42:25 - loss: 0.6904 - accuracy: 0.6000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1220 22:34:05.164333 4382574016 callbacks.py:244] Method (on_train_batch_end) is slow compared to the batch update (1.211758). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8844/8844 [==============================] - 5s 516us/sample - loss: 0.2232 - accuracy: 0.9097\n",
      "Epoch 2/100\n",
      "8844/8844 [==============================] - 2s 211us/sample - loss: 0.1720 - accuracy: 0.9300\n",
      "Epoch 3/100\n",
      "8844/8844 [==============================] - 2s 184us/sample - loss: 0.1516 - accuracy: 0.9376\n",
      "Epoch 4/100\n",
      "8844/8844 [==============================] - 2s 246us/sample - loss: 0.1335 - accuracy: 0.9426\n",
      "Epoch 5/100\n",
      "8844/8844 [==============================] - 2s 204us/sample - loss: 0.1206 - accuracy: 0.9509\n",
      "Epoch 6/100\n",
      "8844/8844 [==============================] - 2s 192us/sample - loss: 0.1135 - accuracy: 0.9508\n",
      "Epoch 7/100\n",
      "8844/8844 [==============================] - 2s 183us/sample - loss: 0.1078 - accuracy: 0.9540\n",
      "Epoch 8/100\n",
      "8844/8844 [==============================] - 2s 183us/sample - loss: 0.1008 - accuracy: 0.9564\n",
      "Epoch 9/100\n",
      "8844/8844 [==============================] - 2s 185us/sample - loss: 0.0957 - accuracy: 0.9579\n",
      "Epoch 10/100\n",
      "8844/8844 [==============================] - 2s 191us/sample - loss: 0.0933 - accuracy: 0.9593\n",
      "Epoch 11/100\n",
      "8844/8844 [==============================] - 2s 184us/sample - loss: 0.0895 - accuracy: 0.9613\n",
      "Epoch 12/100\n",
      "8844/8844 [==============================] - 2s 183us/sample - loss: 0.0870 - accuracy: 0.9627\n",
      "Epoch 13/100\n",
      "8844/8844 [==============================] - 2s 235us/sample - loss: 0.0834 - accuracy: 0.9655\n",
      "Epoch 14/100\n",
      "8844/8844 [==============================] - 2s 220us/sample - loss: 0.0805 - accuracy: 0.9665\n",
      "Epoch 15/100\n",
      "8844/8844 [==============================] - 2s 256us/sample - loss: 0.0785 - accuracy: 0.9661\n",
      "Epoch 16/100\n",
      "8844/8844 [==============================] - 2s 218us/sample - loss: 0.0767 - accuracy: 0.9665\n",
      "Epoch 17/100\n",
      "8844/8844 [==============================] - 2s 206us/sample - loss: 0.0732 - accuracy: 0.9689\n",
      "Epoch 18/100\n",
      "8844/8844 [==============================] - 2s 183us/sample - loss: 0.0732 - accuracy: 0.9689\n",
      "Epoch 19/100\n",
      "8844/8844 [==============================] - 2s 181us/sample - loss: 0.0728 - accuracy: 0.9686\n",
      "Epoch 20/100\n",
      "8844/8844 [==============================] - 2s 211us/sample - loss: 0.0692 - accuracy: 0.9688\n",
      "Epoch 21/100\n",
      "8844/8844 [==============================] - 2s 239us/sample - loss: 0.0659 - accuracy: 0.9700\n",
      "Epoch 22/100\n",
      "8844/8844 [==============================] - 2s 183us/sample - loss: 0.0680 - accuracy: 0.9691\n",
      "Epoch 23/100\n",
      "8844/8844 [==============================] - 3s 310us/sample - loss: 0.0634 - accuracy: 0.9708\n",
      "Epoch 24/100\n",
      "8844/8844 [==============================] - 2s 254us/sample - loss: 0.0667 - accuracy: 0.9705\n",
      "Epoch 25/100\n",
      "8844/8844 [==============================] - 3s 299us/sample - loss: 0.0629 - accuracy: 0.9743\n",
      "Epoch 26/100\n",
      "8844/8844 [==============================] - 3s 292us/sample - loss: 0.0626 - accuracy: 0.9735\n",
      "Epoch 27/100\n",
      "8844/8844 [==============================] - 2s 279us/sample - loss: 0.0582 - accuracy: 0.9734\n",
      "Epoch 28/100\n",
      "8844/8844 [==============================] - 3s 289us/sample - loss: 0.0623 - accuracy: 0.9726\n",
      "Epoch 29/100\n",
      "8844/8844 [==============================] - 2s 282us/sample - loss: 0.0573 - accuracy: 0.9743\n",
      "Epoch 30/100\n",
      "8844/8844 [==============================] - 2s 258us/sample - loss: 0.0561 - accuracy: 0.9764\n",
      "Epoch 31/100\n",
      "8844/8844 [==============================] - 2s 221us/sample - loss: 0.0558 - accuracy: 0.9754\n",
      "Epoch 32/100\n",
      "8844/8844 [==============================] - 2s 251us/sample - loss: 0.0572 - accuracy: 0.9734\n",
      "Epoch 33/100\n",
      "8844/8844 [==============================] - 2s 226us/sample - loss: 0.0636 - accuracy: 0.9741\n",
      "Epoch 34/100\n",
      "8844/8844 [==============================] - 2s 252us/sample - loss: 0.0524 - accuracy: 0.9775\n",
      "Epoch 35/100\n",
      "8844/8844 [==============================] - 2s 244us/sample - loss: 0.0536 - accuracy: 0.9769\n",
      "Epoch 36/100\n",
      "8844/8844 [==============================] - 2s 195us/sample - loss: 0.0523 - accuracy: 0.9755\n",
      "Epoch 37/100\n",
      "8844/8844 [==============================] - 2s 197us/sample - loss: 0.0535 - accuracy: 0.9763\n",
      "Epoch 38/100\n",
      "8844/8844 [==============================] - 2s 210us/sample - loss: 0.0522 - accuracy: 0.9767\n",
      "Epoch 39/100\n",
      "8844/8844 [==============================] - 2s 199us/sample - loss: 0.0491 - accuracy: 0.9783\n",
      "Epoch 40/100\n",
      "8844/8844 [==============================] - 2s 184us/sample - loss: 0.0492 - accuracy: 0.9783\n",
      "Epoch 41/100\n",
      "8844/8844 [==============================] - 2s 240us/sample - loss: 0.0514 - accuracy: 0.9796\n",
      "Epoch 42/100\n",
      "8844/8844 [==============================] - 2s 238us/sample - loss: 0.0498 - accuracy: 0.9785\n",
      "Epoch 43/100\n",
      "8844/8844 [==============================] - 2s 180us/sample - loss: 0.0485 - accuracy: 0.9793\n",
      "Epoch 44/100\n",
      "8844/8844 [==============================] - 2s 174us/sample - loss: 0.0482 - accuracy: 0.9785\n",
      "Epoch 45/100\n",
      "8844/8844 [==============================] - 1s 166us/sample - loss: 0.0478 - accuracy: 0.9783\n",
      "Epoch 46/100\n",
      "8844/8844 [==============================] - 2s 187us/sample - loss: 0.0490 - accuracy: 0.9787\n",
      "Epoch 47/100\n",
      "8844/8844 [==============================] - 2s 187us/sample - loss: 0.0471 - accuracy: 0.9790\n",
      "Epoch 48/100\n",
      "8844/8844 [==============================] - 2s 202us/sample - loss: 0.0452 - accuracy: 0.9809 - loss: 0.0436 \n",
      "Epoch 49/100\n",
      "8844/8844 [==============================] - 1s 167us/sample - loss: 0.0468 - accuracy: 0.9802\n",
      "Epoch 50/100\n",
      "8844/8844 [==============================] - 1s 168us/sample - loss: 0.0455 - accuracy: 0.9796\n",
      "Epoch 51/100\n",
      "8844/8844 [==============================] - 1s 168us/sample - loss: 0.0434 - accuracy: 0.9800\n",
      "Epoch 52/100\n",
      "8844/8844 [==============================] - 1s 169us/sample - loss: 0.0447 - accuracy: 0.9807\n",
      "Epoch 53/100\n",
      "8844/8844 [==============================] - 1s 164us/sample - loss: 0.0450 - accuracy: 0.9796\n",
      "Epoch 54/100\n",
      "8844/8844 [==============================] - 1s 165us/sample - loss: 0.0425 - accuracy: 0.9824\n",
      "Epoch 55/100\n",
      "8844/8844 [==============================] - 2s 183us/sample - loss: 0.0454 - accuracy: 0.9810\n",
      "Epoch 56/100\n",
      "8844/8844 [==============================] - 2s 184us/sample - loss: 0.0428 - accuracy: 0.9817\n",
      "Epoch 57/100\n",
      "8844/8844 [==============================] - 2s 191us/sample - loss: 0.0417 - accuracy: 0.9815\n",
      "Epoch 58/100\n",
      "8844/8844 [==============================] - 2s 213us/sample - loss: 0.0408 - accuracy: 0.9817\n",
      "Epoch 59/100\n",
      "8844/8844 [==============================] - 2s 200us/sample - loss: 0.0422 - accuracy: 0.9817\n",
      "Epoch 60/100\n",
      "8844/8844 [==============================] - 2s 190us/sample - loss: 0.0417 - accuracy: 0.9811\n",
      "Epoch 61/100\n",
      "8844/8844 [==============================] - 2s 179us/sample - loss: 0.0406 - accuracy: 0.9818\n",
      "Epoch 62/100\n",
      "8844/8844 [==============================] - 2s 176us/sample - loss: 0.0426 - accuracy: 0.9800\n",
      "Epoch 63/100\n",
      "8844/8844 [==============================] - 2s 186us/sample - loss: 0.0398 - accuracy: 0.9818\n",
      "Epoch 64/100\n",
      "8844/8844 [==============================] - 2s 191us/sample - loss: 0.0398 - accuracy: 0.9836\n",
      "Epoch 65/100\n",
      "8844/8844 [==============================] - 2s 201us/sample - loss: 0.0403 - accuracy: 0.9821\n",
      "Epoch 66/100\n",
      "8844/8844 [==============================] - 2s 201us/sample - loss: 0.0375 - accuracy: 0.9836\n",
      "Epoch 67/100\n",
      "8844/8844 [==============================] - 1s 167us/sample - loss: 0.0460 - accuracy: 0.9791\n",
      "Epoch 68/100\n",
      "8844/8844 [==============================] - 2s 179us/sample - loss: 0.0493 - accuracy: 0.9803\n",
      "Epoch 69/100\n",
      "8844/8844 [==============================] - 2s 193us/sample - loss: 0.0352 - accuracy: 0.9826\n",
      "Epoch 70/100\n",
      "8844/8844 [==============================] - 2s 183us/sample - loss: 0.0379 - accuracy: 0.9825\n",
      "Epoch 71/100\n",
      "8844/8844 [==============================] - 2s 212us/sample - loss: 0.0386 - accuracy: 0.9824\n",
      "Epoch 72/100\n",
      "8844/8844 [==============================] - 2s 191us/sample - loss: 0.0409 - accuracy: 0.9808\n",
      "Epoch 73/100\n",
      "8844/8844 [==============================] - 2s 183us/sample - loss: 0.0377 - accuracy: 0.9825\n",
      "Epoch 74/100\n",
      "8630/8844 [============================>.] - ETA: 0s - loss: 0.0373 - accuracy: 0.9830Restoring model weights from the end of the best epoch.\n",
      "8844/8844 [==============================] - 2s 189us/sample - loss: 0.0373 - accuracy: 0.9830\n",
      "Epoch 00074: early stopping\n",
      "Accuracy score of the Logistic Regression classifier with default hyperparameter values 98.51%\n",
      "\n",
      "\n",
      "----Classification report of the Logistic Regression classifier with default hyperparameter value----\n",
      "\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Phishing Websites       0.99      0.97      0.98      3924\n",
      "  Normal Websites       0.98      0.99      0.99      4920\n",
      "\n",
      "        micro avg       0.99      0.99      0.99      8844\n",
      "        macro avg       0.99      0.98      0.98      8844\n",
      "     weighted avg       0.99      0.99      0.99      8844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history_TDLHBA = model_TDLHBA.fit(X_train, y_train, batch_size=10, epochs=100, \n",
    "                                  verbose=1, callbacks=[es_cb, WandbCallback()])\n",
    "\n",
    "end = time.time() - start\n",
    "prediction = model_TDLHBA.predict_classes(X_test)\n",
    "wandb.log({\"accuracy\":accuracy_score(y_test, prediction)*100.0,\\\n",
    "           \"precision\": precision_recall_fscore_support(y_test, prediction, average=\"macro\")[0],\n",
    "           \"recall\": precision_recall_fscore_support(y_test, prediction, average=\"macro\")[1],\n",
    "           \"training_time\":end})\n",
    "print(\"Accuracy score of the Logistic Regression classifier with default hyperparameter values {0:.2f}%\"\\\n",
    "                  .format(accuracy_score(y_test, prediction)*100.))\n",
    "print(\"\\n\")\n",
    "print(\"----Classification report of the Logistic Regression classifier with default hyperparameter value----\")\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test, prediction, target_names=[\"Phishing Websites\", \"Normal Websites\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is by far the best one. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
